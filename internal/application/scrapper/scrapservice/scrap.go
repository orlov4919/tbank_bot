package scrapservice

import (
	"context"
	"linkTraccer/internal/domain/scrapper"
	"log/slog"
	"time"
)

const (
	descriptionFormat = "–ü—Ä–∏—à–ª–æ –Ω–æ–≤–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ üî•\n\n–°–æ–±—ã—Ç–∏–µ: %s\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: %s\n–í—Ä–µ–º—è —Å–æ–∑–¥–∞–µ–Ω–∏—è: %s\n–ü—Ä–µ–≤—å—é: %s\n\n"
)

var (
	MoskowTime = time.FixedZone("UTC+3", 3*60*60)
)

type User = scrapper.User
type Link = scrapper.Link
type LinkState = scrapper.LinkState
type LinkInfo = scrapper.LinkInfo
type LinkUpdates = scrapper.LinkUpdates
type LinkID = scrapper.LinkID
type LinkUpdate = scrapper.LinkUpdate

type LinkPaginator interface {
	LinksBatch() ([]LinkInfo, error)
	//HasNext() bool // –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –±—ã–ª –±—ã –∫—Ä—É—Ç, –Ω–æ —Ç–∞–∫ —Ö—ã–∑
}

type UserRepo interface {
	NewLinksPaginator() LinkPaginator
	TrackLink(ctx context.Context, userID User, link Link, update time.Time) error
	ChangeLastCheckTime(link Link, checkTime time.Time) error
	UsersWhoTrackLink(linkID LinkID) ([]User, error)
	AllUserLinks(userID User) ([]Link, error)
	UserTrackLink(userID User, URL Link) (bool, error)
	UntrackLink(user User, link Link) error
	UserExist(UserID User) (bool, error)
	RegUser(UserID User) error
	DeleteUser(ctx context.Context, user User) error
}

type SiteClient interface {
	CanTrack(link Link) bool
	LinkUpdates(link Link, updatesSince time.Time) (LinkUpdates, error)
}

type NotifyService interface {
	SendUpdates(linkInfo LinkInfo, linkUpdates LinkUpdates) error
}

type Transactor interface {
	WithTransaction(ctx context.Context, fn func(ctx context.Context) error) error
}

type Scrapper struct {
	userRepo      UserRepo
	siteClients   []SiteClient
	notifyService NotifyService
	log           *slog.Logger
}

func New(userRepo UserRepo, notifyService NotifyService, log *slog.Logger, siteClients ...SiteClient) *Scrapper {
	return &Scrapper{
		userRepo:      userRepo,
		notifyService: notifyService,
		siteClients:   siteClients,
		log:           log,
	}
}

func (scrap *Scrapper) CheckLinksUpdates() {
	linksPaginator := scrap.userRepo.NewLinksPaginator()
	links, err := linksPaginator.LinksBatch()

	for len(links) != 0 && err == nil {
		for _, linkInfo := range links {
			for _, siteClient := range scrap.siteClients {

				if !siteClient.CanTrack(linkInfo.URL) {
					continue
				}

				t := time.Now().In(MoskowTime).Truncate(time.Second)

				linkUpdates, err := siteClient.LinkUpdates(linkInfo.URL, linkInfo.LastUpdate)

				if err != nil {
					scrap.log.Info("–ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å—Å—ã–ª–∫–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞", "err", err.Error())
					break
				}

				if err = scrap.userRepo.ChangeLastCheckTime(linkInfo.URL, t); err != nil {
					scrap.log.Info("–æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –¥–∞—Ç—ã –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Å—ã–ª–∫–∏", "err", err.Error())
					break
				}

				if len(linkUpdates) == 0 {
					break
				}

				scrap.log.Info("—Å—Å—ã–ª–∫–∞ " + linkInfo.URL + " –ø–æ–ª—É—á–∏–ª–∞ –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ")

				if err = scrap.notifyService.SendUpdates(linkInfo, linkUpdates); err != nil {
					scrap.log.Info("–æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π", "err", err.Error())
				}
			}
		}

		links, err = linksPaginator.LinksBatch()
	}

	if err != nil {
		scrap.log.Info("–æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å—Å—ã–ª–æ–∫", "err", err.Error())
	}
}
